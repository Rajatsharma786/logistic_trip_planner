{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eb65576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredPDFLoader\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import fitz \n",
    "import os, json, gzip, sys, traceback\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import base64\n",
    "import os\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type\n",
    ")\n",
    "import time\n",
    "from openai import RateLimitError\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.auto import tqdm\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c09b1613",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_PATH = r\"C:\\Users\\DELL\\Desktop\\AI Projects\\langchain_proj\\Logistic_Agent_Planner\\data\\road_knowledge\"\n",
    "CACHE_DIR   = r\"C:\\Users\\DELL\\Desktop\\AI Projects\\Langchain_proj\\Logistic_Agent_Planner\\notebooks\\cache_json\"\n",
    "FIG_DIR     = r\"C:\\Users\\DELL\\Desktop\\AI Projects\\Langchain_proj\\Logistic_Agent_Planner\\notebooks\\figures\"      \n",
    "COMPRESS    = True                          \n",
    "OVERWRITE   = False                          \n",
    "LOG_EVERY   = 1  \n",
    "PERSIST_DIR = \"./chroma_text\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00c25147",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import camelot\n",
    "    _CAMELOT_AVAILABLE = True\n",
    "except Exception:\n",
    "    camelot = None\n",
    "    _CAMELOT_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac792e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl(records, out_path, compress: bool = True):\n",
    "    out_path = Path(out_path)                   # <— ensure Path\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if compress and not str(out_path).endswith(\".gz\"):\n",
    "        out_path = out_path.with_suffix(out_path.suffix + \".gz\")\n",
    "    import gzip, json\n",
    "    opener = gzip.open if str(out_path).endswith(\".gz\") else open\n",
    "    with opener(out_path, \"wt\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            json.dump(r, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")\n",
    "    return out_path\n",
    "\n",
    "def load_jsonl(path: Path):\n",
    "    opener = gzip.open if str(path).endswith(\".gz\") else open\n",
    "    with opener(path, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                yield json.loads(line)\n",
    "\n",
    "def is_text_pdf(pdf_path: str, sample_pages: int = 2) -> bool:\n",
    "    \"\"\"Cheap probe: if PyPDFLoader returns some text, treat as text-native.\"\"\"\n",
    "    try:\n",
    "        docs = PyPDFLoader(pdf_path).load()\n",
    "        s = \"\".join(d.page_content for d in docs[:sample_pages])\n",
    "        return len(s.strip()) > 50\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "122b0e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_fast(pdf_path: str):\n",
    "    \"\"\"Use PyPDFLoader for text-native PDFs (fast). Return list[Document].\"\"\"\n",
    "    return PyPDFLoader(pdf_path).load()\n",
    "\n",
    "def load_text_ocr(pdf_path: str):\n",
    "    \"\"\"Light OCR via unstructured (no heavy layout model).\"\"\"\n",
    "    return UnstructuredPDFLoader(\n",
    "        file_path=pdf_path,\n",
    "        strategy=\"fast\",            # MUCH faster than 'hi_res'\n",
    "        mode=\"elements\",\n",
    "        extract_images_in_pdf=False,\n",
    "        infer_table_structure=False\n",
    "    ).load()\n",
    "\n",
    "def sniff_pages_with_layout(pdf_path: str):\n",
    "    \"\"\"Use unstructured 'fast' to find which pages have tables/images.\"\"\"\n",
    "    tables, figures = set(), set()\n",
    "    try:\n",
    "        elements = partition_pdf(pdf_path, strategy=\"fast\", include_page_breaks=True)\n",
    "        for el in elements:\n",
    "            # unstructured elements have .category and .metadata.page_number\n",
    "            cat = getattr(el, \"category\", \"\") or el.__class__.__name__\n",
    "            pg  = getattr(getattr(el, \"metadata\", None), \"page_number\", None)\n",
    "            if not pg:\n",
    "                continue\n",
    "            cat_l = str(cat).lower()\n",
    "            if \"table\" in cat_l:\n",
    "                tables.add(pg)\n",
    "            if \"image\" in cat_l or \"figure\" in cat_l:\n",
    "                figures.add(pg)\n",
    "    except Exception:\n",
    "        # If detection fails, leave sets empty and let downstream skip heavy work\n",
    "        pass\n",
    "    return {\"tables\": tables, \"figures\": figures}\n",
    "\n",
    "def extract_tables(pdf_path: str, pages: set[int]):\n",
    "    \"\"\"Return list of dicts with page + 2D table data. Gracefully degrades if Camelot unavailable.\"\"\"\n",
    "    results = []\n",
    "    if not pages or not _CAMELOT_AVAILABLE:\n",
    "        return results\n",
    "    page_str = \",\".join(map(str, sorted(pages)))\n",
    "    try:\n",
    "        # Try lattice (lines). If none found, try stream (whitespace).\n",
    "        tables = camelot.read_pdf(pdf_path, pages=page_str, flavor=\"lattice\")\n",
    "        if len(tables) == 0:\n",
    "            tables = camelot.read_pdf(pdf_path, pages=page_str, flavor=\"stream\")\n",
    "        for t in tables:\n",
    "            try:\n",
    "                results.append({\n",
    "                    \"type\": \"table\",\n",
    "                    \"page\": int(t.parsing_report.get(\"page\", -1)),\n",
    "                    \"shape\": list(t.shape),\n",
    "                    \"data\": t.df.values.tolist()\n",
    "                })\n",
    "            except Exception:\n",
    "                # Fallback if parsing_report missing\n",
    "                results.append({\n",
    "                    \"type\": \"table\",\n",
    "                    \"page\": None,\n",
    "                    \"shape\": list(getattr(t, \"shape\", (-1, -1))),\n",
    "                    \"data\": t.df.values.tolist()\n",
    "                })\n",
    "    except Exception:\n",
    "        # If Camelot errors (e.g., Ghostscript missing), skip tables\n",
    "        pass\n",
    "    return results\n",
    "\n",
    "def extract_images(pdf_path: str, pages: set[int], out_dir: Path = FIG_DIR):\n",
    "    \"\"\"Extract embedded images with PyMuPDF from selected pages.\"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    results = []\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for pno in sorted(pages):\n",
    "                if pno < 1 or pno > doc.page_count: \n",
    "                    continue\n",
    "                page = doc[pno-1]\n",
    "                for img in page.get_images(full=True):\n",
    "                    xref = img[0]\n",
    "                    pix = fitz.Pixmap(doc, xref)\n",
    "                    name = f\"{Path(pdf_path).stem}_p{pno}_{xref}.png\"\n",
    "                    save_path = out_dir / name\n",
    "                    if pix.n < 5:      # RGB or Gray\n",
    "                        pix.save(save_path)\n",
    "                    else:              # CMYK: convert\n",
    "                        fitz.Pixmap(fitz.csRGB, pix).save(save_path)\n",
    "                    results.append({\"type\": \"figure\", \"page\": pno, \"path\": str(save_path)})\n",
    "                    pix = None\n",
    "    except Exception:\n",
    "        pass\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "610bcb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_to_text_records(docs):\n",
    "    \"\"\"LangChain Documents -> JSONL records with page & metadata preserved.\"\"\"\n",
    "    records = []\n",
    "    for d in docs:\n",
    "        md = dict(d.metadata or {})\n",
    "        page = md.get(\"page\", md.get(\"page_number\", None))\n",
    "        records.append({\n",
    "            \"type\": \"text\",\n",
    "            \"page\": page,\n",
    "            \"content\": d.page_content,\n",
    "            \"metadata\": md\n",
    "        })\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55c6e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path, cache_dir=None, overwrite=False):\n",
    "    pdf_path  = Path(pdf_path)                  # <— ensure Path\n",
    "    cache_dir = Path(cache_dir) if cache_dir else Path(\"./cache_json\")\n",
    "    out_path  = cache_dir / (pdf_path.stem + \".jsonl\")\n",
    "\n",
    "    if out_path.with_suffix(out_path.suffix + \".gz\").exists() or out_path.exists():\n",
    "        if not overwrite:\n",
    "            return str(out_path) if out_path.exists() else str(out_path) + \".gz\"\n",
    "\n",
    "    records = []\n",
    "\n",
    "    # 1) TEXT\n",
    "    try:\n",
    "        if is_text_pdf(str(pdf_path)):\n",
    "            docs = load_text_fast(str(pdf_path))\n",
    "        else:\n",
    "            docs = load_text_ocr(str(pdf_path))\n",
    "        records.extend(docs_to_text_records(docs))\n",
    "    except Exception as e:\n",
    "        print(f\"[TEXT ERROR] {pdf_path}: {e}\")\n",
    "\n",
    "    # 2) LAYOUT SNIFF\n",
    "    pages = sniff_pages_with_layout(str(pdf_path))\n",
    "    table_pages, figure_pages = pages[\"tables\"], pages[\"figures\"]\n",
    "\n",
    "    # 3) TABLES & 4) IMAGES\n",
    "    records.extend(extract_tables(str(pdf_path), table_pages))\n",
    "    records.extend(extract_images(str(pdf_path), figure_pages, Path(\"./figures\")))\n",
    "\n",
    "    # 5) SAVE\n",
    "    saved_path = save_jsonl(records, out_path, compress=True)\n",
    "    return str(saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c71ae64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch(folder):\n",
    "    folder = Path(folder)                       # <— ensure Path\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "    pdfs = sorted(p for p in folder.glob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdfs)} PDFs.\")\n",
    "    for i, pdf in enumerate(pdfs, 1):\n",
    "        target = process_pdf(pdf, cache_dir=Path(\"./cache_json\"), overwrite=False)\n",
    "        print(f\"[{i}/{len(pdfs)}] cached → {target}\")\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ed08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_batch(FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7000ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directory exists: True\n",
      "Files in cache directory: [WindowsPath('C:/Users/DELL/Desktop/AI Projects/Langchain_proj/Logistic_Agent_Planner/notebooks/cache_json/250210-IR-Full-Alignment-Map-Web-1-1.jsonl.gz'), WindowsPath('C:/Users/DELL/Desktop/AI Projects/Langchain_proj/Logistic_Agent_Planner/notebooks/cache_json/alternative-heavy-vehicle-rest-area-hume-motorway-minor-works-ref.jsonl.gz'), WindowsPath('C:/Users/DELL/Desktop/AI Projects/Langchain_proj/Logistic_Agent_Planner/notebooks/cache_json/Appendix B - Resilience analysis of key freight roads and railways using CSIRO’s TraNSIT.jsonl.gz'), WindowsPath('C:/Users/DELL/Desktop/AI Projects/Langchain_proj/Logistic_Agent_Planner/notebooks/cache_json/Chapter-2-Project-Rationale.jsonl.gz'), WindowsPath('C:/Users/DELL/Desktop/AI Projects/Langchain_proj/Logistic_Agent_Planner/notebooks/cache_json/Factsheet-Warnings-pdf.jsonl.gz'), WindowsPath('C:/Users/DELL/Desktop/AI Projects/Langchain_proj/Logistic_Agent_Planner/notebooks/cache_json/Freight-Policy-Reform-Consultation-Paper-April-2024.jsonl.gz'), WindowsPath('C:/Users/DELL/Desktop/AI Projects/Langchain_proj/Logistic_Agent_Planner/notebooks/cache_json/Hume-Region-Planning-for-Freight-Pilot-Strategy-Report.jsonl.gz'), WindowsPath('C:/Users/DELL/Desktop/AI Projects/Langchain_proj/Logistic_Agent_Planner/notebooks/cache_json/Hume-Regional-Growth-Plan-May-2014.jsonl.gz'), WindowsPath('C:/Users/DELL/Desktop/AI Projects/Langchain_proj/Logistic_Agent_Planner/notebooks/cache_json/RAMJO-Regional-Freight-Transport-Plan-Version-3-final-document-1.jsonl.gz'), WindowsPath('C:/Users/DELL/Desktop/AI Projects/Langchain_proj/Logistic_Agent_Planner/notebooks/cache_json/Road and Rail Supply Chain Resilience Review_Final.jsonl.gz'), WindowsPath('C:/Users/DELL/Desktop/AI Projects/Langchain_proj/Logistic_Agent_Planner/notebooks/cache_json/sa202402-1481-national-level-crossing-safety-notice.jsonl.gz')]\n",
      "Processing file: C:\\Users\\DELL\\Desktop\\AI Projects\\Langchain_proj\\Logistic_Agent_Planner\\notebooks\\cache_json\\250210-IR-Full-Alignment-Map-Web-1-1.jsonl.gz\n",
      "Processing file: C:\\Users\\DELL\\Desktop\\AI Projects\\Langchain_proj\\Logistic_Agent_Planner\\notebooks\\cache_json\\alternative-heavy-vehicle-rest-area-hume-motorway-minor-works-ref.jsonl.gz\n",
      "Processing file: C:\\Users\\DELL\\Desktop\\AI Projects\\Langchain_proj\\Logistic_Agent_Planner\\notebooks\\cache_json\\Appendix B - Resilience analysis of key freight roads and railways using CSIRO’s TraNSIT.jsonl.gz\n",
      "Processing file: C:\\Users\\DELL\\Desktop\\AI Projects\\Langchain_proj\\Logistic_Agent_Planner\\notebooks\\cache_json\\Chapter-2-Project-Rationale.jsonl.gz\n",
      "Processing file: C:\\Users\\DELL\\Desktop\\AI Projects\\Langchain_proj\\Logistic_Agent_Planner\\notebooks\\cache_json\\Factsheet-Warnings-pdf.jsonl.gz\n",
      "Processing file: C:\\Users\\DELL\\Desktop\\AI Projects\\Langchain_proj\\Logistic_Agent_Planner\\notebooks\\cache_json\\Freight-Policy-Reform-Consultation-Paper-April-2024.jsonl.gz\n",
      "Processing file: C:\\Users\\DELL\\Desktop\\AI Projects\\Langchain_proj\\Logistic_Agent_Planner\\notebooks\\cache_json\\Hume-Region-Planning-for-Freight-Pilot-Strategy-Report.jsonl.gz\n",
      "Processing file: C:\\Users\\DELL\\Desktop\\AI Projects\\Langchain_proj\\Logistic_Agent_Planner\\notebooks\\cache_json\\Hume-Regional-Growth-Plan-May-2014.jsonl.gz\n",
      "Processing file: C:\\Users\\DELL\\Desktop\\AI Projects\\Langchain_proj\\Logistic_Agent_Planner\\notebooks\\cache_json\\RAMJO-Regional-Freight-Transport-Plan-Version-3-final-document-1.jsonl.gz\n",
      "Processing file: C:\\Users\\DELL\\Desktop\\AI Projects\\Langchain_proj\\Logistic_Agent_Planner\\notebooks\\cache_json\\Road and Rail Supply Chain Resilience Review_Final.jsonl.gz\n",
      "Processing file: C:\\Users\\DELL\\Desktop\\AI Projects\\Langchain_proj\\Logistic_Agent_Planner\\notebooks\\cache_json\\sa202402-1481-national-level-crossing-safety-notice.jsonl.gz\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "cache_dir = Path(CACHE_DIR)\n",
    "print(f\"Cache directory exists: {cache_dir.exists()}\")\n",
    "print(f\"Files in cache directory: {list(cache_dir.glob('*.jsonl*'))}\")  \n",
    "for p in sorted(cache_dir.glob(\"*.jsonl.gz\")):  \n",
    "    print(f\"Processing file: {p}\")\n",
    "    try:\n",
    "        with gzip.open(p, \"rt\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    r = json.loads(line)\n",
    "                    records.append(r)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {p}: {e}\")\n",
    "        print(f\"File content preview: {open(p, 'rb').read()[:100]}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f05c8fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for r in records:\n",
    "    if r[\"type\"] == \"text\" and r.get(\"content\"):\n",
    "        md = r.get(\"metadata\", {})\n",
    "        md.update({\"page\": r.get(\"page\"), \"type\": \"text\"})\n",
    "        docs.append(Document(page_content=r[\"content\"], metadata=md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6610213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0409f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6a4ab58c0341109ecd38a65bb03bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing documents:   0%|          | 0/5597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#benefits of using async\n",
    "# Efficiency: Process multiple documents simultaneously\n",
    "# Rate Limiting: Avoid API throttling\n",
    "# Error Handling: Graceful recovery from failures\n",
    "# Progress Tracking: See processing status\n",
    "# Resource Management: Control concurrent API calls\n",
    "\n",
    "chatgpt = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo-16k',  # Larger context window\n",
    "    temperature=0,\n",
    "    max_retries=3,\n",
    "    request_timeout=60,\n",
    "    max_tokens=500  # Limit summary length\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "prompt_text = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text particularly for semantic retrieval.\n",
    "These summaries will be embedded and used to retrieve the raw text or table elements\n",
    "Give a detailed summary of the table or text below that is well optimized for retrieval.\n",
    "For any tables also add in a one line description of what the table is about besides the summary.\n",
    "Do not add redundant words like Summary.\n",
    "Just output the actual summary content.\n",
    "\n",
    "Table or text chunk:\n",
    "{element}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "summarize_chain = (\n",
    "                    {\"element\": RunnablePassthrough()}\n",
    "                      |\n",
    "                    prompt\n",
    "                      |\n",
    "                    chatgpt\n",
    "                      |\n",
    "                    StrOutputParser() \n",
    ")\n",
    "text_summaries = []\n",
    "BATCH_SIZE = 20\n",
    "MAX_CONCURRENCY = 15\n",
    "\n",
    "text_docs = [doc.page_content for doc in docs]\n",
    "\n",
    "#Using this to handel openai rate limits\n",
    "@retry(\n",
    "    retry=retry_if_exception_type(RateLimitError),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=6),\n",
    "    stop=stop_after_attempt(3)\n",
    ")\n",
    "async def process_batch_async(batch):\n",
    "    return await summarize_chain.abatch(batch, {\"max_concurrency\": MAX_CONCURRENCY})\n",
    "\n",
    "async def process_all_batches():\n",
    "    text_summaries = []\n",
    "    total_batches = (len(text_docs) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    \n",
    "    with tqdm(total=len(text_docs), desc=\"Processing documents\") as pbar:\n",
    "        for i in range(0, len(text_docs), BATCH_SIZE):\n",
    "            batch = text_docs[i:i + BATCH_SIZE]\n",
    "            try:\n",
    "                batch_summaries = await process_batch_async(batch)\n",
    "                text_summaries.extend(batch_summaries)\n",
    "                pbar.update(len(batch))\n",
    "                \n",
    "                # Only sleep if we hit rate limit\n",
    "                if i % (BATCH_SIZE * 3) == 0:\n",
    "                    await asyncio.sleep(1)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError in batch {i//BATCH_SIZE + 1}: {str(e)}\")\n",
    "                await asyncio.sleep(2)  # Longer sleep on error\n",
    "                continue\n",
    "    \n",
    "    return text_summaries\n",
    "\n",
    "async def main():\n",
    "    # Process text documents\n",
    "    text_summaries = await process_all_batches()\n",
    "    return text_summaries\n",
    "\n",
    "# Run the async processing with proper event loop handling\n",
    "\n",
    "nest_asyncio.apply()  # This allows running async code in Jupyter notebooks\n",
    "\n",
    "# Now run the main async function\n",
    "text_summaries = asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dca30f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d0883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added batch of 500 documents (237 pairs)\n",
      "Added batch of 500 documents (487 pairs)\n",
      "Added batch of 500 documents (728 pairs)\n",
      "Added batch of 500 documents (969 pairs)\n",
      "Added batch of 500 documents (1219 pairs)\n",
      "Added batch of 500 documents (1469 pairs)\n",
      "Added batch of 500 documents (1719 pairs)\n",
      "Added batch of 500 documents (1969 pairs)\n",
      "Added batch of 500 documents (2219 pairs)\n",
      "Added batch of 500 documents (2469 pairs)\n",
      "Added batch of 500 documents (2719 pairs)\n",
      "Added batch of 500 documents (2969 pairs)\n",
      "Added batch of 500 documents (3219 pairs)\n",
      "Added batch of 500 documents (3469 pairs)\n",
      "Added batch of 500 documents (3719 pairs)\n",
      "Added batch of 500 documents (3969 pairs)\n",
      "Added batch of 500 documents (4219 pairs)\n",
      "Added batch of 500 documents (4469 pairs)\n",
      "Added batch of 500 documents (4719 pairs)\n",
      "Added batch of 500 documents (4969 pairs)\n",
      "Added batch of 500 documents (5219 pairs)\n",
      "Added batch of 500 documents (5469 pairs)\n",
      "Added batch of 268 documents (5597 pairs)\n",
      "Total: Added 11268 documents (5597 pairs)\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "import uuid\n",
    "\n",
    "# Initialize vector store\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=f\"{PERSIST_DIR}_vectorstore\",\n",
    "    embedding_function=openai_embed_model,\n",
    "    collection_name=\"combined_store\",\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,          # Adjust chunk size based on your needs\n",
    "    chunk_overlap=200,        # Overlap to maintain context between chunks\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try to split on paragraph breaks first\n",
    ")\n",
    "\n",
    "\n",
    "def store_documents_with_summaries(summaries, original_docs, batch_size=500):\n",
    "    \"\"\"Store documents and their summaries with linked UUIDs in batches\"\"\"\n",
    "    documents = []\n",
    "    total_pairs = 0\n",
    "    \n",
    "    # Split original documents into chunks first\n",
    "    for summary, doc in zip(summaries, original_docs):\n",
    "        pair_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Split the original content into chunks\n",
    "        content_chunks = text_splitter.split_text(doc.page_content)\n",
    "        \n",
    "        # Create documents for each chunk\n",
    "        for i, chunk in enumerate(content_chunks):\n",
    "            chunk_doc = Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"pair_id\": pair_id,\n",
    "                    \"chunk_id\": i,\n",
    "                    \"type\": \"content\",\n",
    "                    \"page\": doc.metadata.get(\"page\", None),\n",
    "                    \"is_summary\": False\n",
    "                }\n",
    "            )\n",
    "            documents.append(chunk_doc)\n",
    "        \n",
    "        # Keep summary as a single piece\n",
    "        summary_doc = Document(\n",
    "            page_content=summary,\n",
    "            metadata={\n",
    "                \"pair_id\": pair_id,\n",
    "                \"type\": \"summary\",\n",
    "                \"page\": doc.metadata.get(\"page\", None),\n",
    "                \"is_summary\": True\n",
    "            }\n",
    "        )\n",
    "        documents.append(summary_doc)\n",
    "    \n",
    "    # Add documents in batches\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        vectorstore.add_documents(batch)\n",
    "        total_pairs += len([d for d in batch if d.metadata[\"is_summary\"]])\n",
    "        print(f\"Added batch of {len(batch)} documents ({total_pairs} pairs)\")\n",
    "    \n",
    "    print(f\"Total: Added {len(documents)} documents ({total_pairs} pairs)\")\n",
    "    return documents\n",
    "\n",
    "try:\n",
    "    stored_docs = store_documents_with_summaries(\n",
    "        summaries=text_summaries,\n",
    "        original_docs=docs,\n",
    "        batch_size=500  # Adjust this value if needed\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error storing documents: {str(e)}\")\n",
    "\n",
    "\n",
    "# Create retriever with compression\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 6,              # Increased to get more chunks\n",
    "        \"fetch_k\": 30,       # Fetch more candidates for MMR\n",
    "        \"lambda_mult\": 0.7   # Balance between relevance and diversity\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the final retriever with compression\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "final_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")\n",
    "\n",
    "# Save the vectorstore\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8af2fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing QA System:\n",
      "--------------------------------------------------\n",
      "\n",
      "Question: Give me all information you have about hume highway\n",
      "Answer: The provided context repeatedly mentions the Hume Highway (HW2) and indicates that it is located on page 33.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gpt = ChatOpenAI(model='gpt-4o-mini', temperature=0.4)\n",
    "qa_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant that provides accurate answers based on the given context.\n",
    "Answer the question using only the provided context. If you cannot find the answer in the context, say \"I cannot answer this based on the provided context.\"\n",
    "Be concise and clear in your response. repharse the response in more human readable way.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\")\n",
    "\n",
    "# Define formatting for the retrieved documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | qa_prompt\n",
    "    | gpt\n",
    ")\n",
    "\n",
    "# Function to get answers\n",
    "def get_answer(question: str) -> str:\n",
    "    \"\"\"Get answer for a specific question using the RAG chain\"\"\"\n",
    "    response = rag_chain.invoke(question)\n",
    "    return response.content\n",
    "\n",
    "# Example usage and testing\n",
    "test_questions = [\n",
    "    \"Give me all information you have about hume highway\",\n",
    "]\n",
    "\n",
    "print(\"Testing QA System:\")\n",
    "print(\"-\" * 50)\n",
    "for question in test_questions:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Answer: {get_answer(question)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a13de68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
